{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rBOGK61EVk2VSx3PtPH87sxZUuhyKO6O",
      "authorship_tag": "ABX9TyP+ZXwf1a1IhKDj2z8y2/IW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its-sisila/ModelX_Competition_T161/blob/main/modelx_t161.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. SETUP AND IMPORT LIBRARIES ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# import shap # SHAP IS REMOVED\n",
        "import warnings\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"--- 1. Libraries Imported ---\")\n",
        "\n",
        "# --- 2. MOUNT GOOGLE DRIVE ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"--- Google Drive Mounted Successfully ---\")\n",
        "except ImportError:\n",
        "    print(\"--- Not running in Colab, assuming file is local ---\")\n",
        "\n",
        "# --- 3. LOAD DATA ---\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/Dementia Prediction Dataset.csv'\n",
        "\n",
        "columns_to_load = [\n",
        "    'VISITYR',  # Needed to calculate AGE\n",
        "    'BIRTHYR',  # Needed to calculate AGE\n",
        "    'EDUC',     # Education\n",
        "    'SEX',      # Sex\n",
        "    'MARISTAT', # Marital Status (was 'MARRIED')\n",
        "    'RACE',     # Race\n",
        "    'HANDED',   # Handedness (was 'HAND')\n",
        "    'NACCALZD'  # THE REAL TARGET (was 'NACCADC' or 'DX')\n",
        "]\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(filepath, low_memory=False, usecols=columns_to_load)\n",
        "    print(f\"--- 3. Successfully loaded required columns from: {filepath} ---\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Could not find file at '{filepath}'.\")\n",
        "    exit()\n",
        "except ValueError as e:\n",
        "    print(f\"ERROR: A column might be misspelled. Check list: {columns_to_load}\")\n",
        "    print(f\"Full error: {e}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during file loading: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 4. FEATURE ENGINEERING & SELECTION ---\n",
        "\n",
        "print(\"--- 4. Engineering 'AGE' feature (VISITYR - BIRTHYR) ---\")\n",
        "df['AGE'] = df['VISITYR'] - df['BIRTHYR']\n",
        "\n",
        "allowed_features = [\n",
        "    'AGE',      # Our new engineered feature\n",
        "    'EDUC',\n",
        "    'SEX',\n",
        "    'MARISTAT',\n",
        "    'RACE',\n",
        "    'HANDED'\n",
        "]\n",
        "target_variable = 'NACCALZD'\n",
        "print(\"--- 4. Feature Engineering Complete. ---\")\n",
        "\n",
        "# --- 5. DATA CLEANING & PREPROCESSING ---\n",
        "print(\"--- 5. Cleaning Data... ---\")\n",
        "\n",
        "codes_normal = [8]      # 8 = No cognitive impairment\n",
        "codes_dementia = [1]    # 1 = Yes (Dementia)\n",
        "\n",
        "df_filtered = df[df[target_variable].isin(codes_normal + codes_dementia)].copy()\n",
        "df_filtered['target'] = df_filtered[target_variable].map(lambda x: 0 if x in codes_normal else 1)\n",
        "\n",
        "print(\"\\nFiltered Target value counts (binary):\")\n",
        "print(df_filtered['target'].value_counts())\n",
        "\n",
        "df_filtered[allowed_features] = df_filtered[allowed_features].replace(-4, np.nan)\n",
        "\n",
        "X = df_filtered[allowed_features]\n",
        "y = df_filtered['target']\n",
        "\n",
        "numeric_features = ['AGE', 'EDUC']\n",
        "categorical_features = ['SEX', 'MARISTAT', 'RACE', 'HANDED']\n",
        "\n",
        "print(\"--- Data Cleaned and Processed. ---\")\n",
        "\n",
        "# --- 6. BUILD PREPROCESSING PIPELINES ---\n",
        "print(\"--- 6. Building Preprocessing Pipelines... ---\")\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- 7. TRAIN-TEST SPLIT ---\n",
        "print(\"--- 7. Splitting Data into Train/Test... ---\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "# --- 8. MODEL BUILDING ---\n",
        "\n",
        "print(\"\\nTraining Logistic Regression...\")\n",
        "lr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', LogisticRegression(random_state=42, solver='liblinear'))\n",
        "])\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nTraining and Tuning Random Forest (GridSearchCV)...\")\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [10, 20],\n",
        "    'model__min_samples_leaf': [2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='f1',\n",
        "    verbose=1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "final_model = grid_search.best_estimator_\n",
        "print(f\"\\nBest Random Forest Params: {grid_search.best_params_}\")\n",
        "print(\"--- 8. Model training complete. ---\")\n",
        "\n",
        "# --- 9. MODEL EVALUATION ---\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*30)\n",
        "print(\"   MODEL EVALUATION RESULTS   \")\n",
        "print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "print(\"--- Logistic Regression (Baseline) ---\")\n",
        "y_pred_lr = lr_pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_lr, target_names=['Normal (0)', 'Dementia (1)']))\n",
        "f1_lr = f1_score(y_test, y_pred_lr, pos_label=1)\n",
        "\n",
        "print(\"--- Tuned Random Forest (Final Model) ---\")\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_final, target_names=['Normal (0)', 'Dementia (1)']))\n",
        "f1_rf = f1_score(y_test, y_pred_final, pos_label=1)\n",
        "\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "evaluation_data = {\n",
        "    \"Metric\": [\"Accuracy\", \"F1-Score (Dementia)\"],\n",
        "    \"Logistic Regression\": [\n",
        "        accuracy_score(y_test, y_pred_lr),\n",
        "        f1_lr\n",
        "    ],\n",
        "    \"Tuned Random Forest\": [\n",
        "        accuracy_score(y_test, y_pred_final),\n",
        "        f1_rf\n",
        "    ]\n",
        "}\n",
        "eval_df = pd.DataFrame(evaluation_data).set_index(\"Metric\")\n",
        "print(eval_df.to_markdown(floatfmt=\".4f\"))\n",
        "\n",
        "# --- 10. NEW: VERBAL EXPLAINABILITY ---\n",
        "print(\"\\n--- 10. Generating Feature Importances ---\")\n",
        "\n",
        "try:\n",
        "    # Get the pipeline steps\n",
        "    preprocessor_step = final_model.named_steps['preprocessor']\n",
        "    model_step = final_model.named_steps['model']\n",
        "\n",
        "    # Get the feature names from the OneHotEncoder\n",
        "    ohe_feature_names = preprocessor_step.named_transformers_['cat'] \\\n",
        "        .named_steps['onehot'] \\\n",
        "        .get_feature_names_out(categorical_features)\n",
        "\n",
        "    # Combine all feature names in the correct order\n",
        "    all_feature_names = numeric_features + list(ohe_feature_names)\n",
        "\n",
        "    # Get the importances from the trained model\n",
        "    importances = model_step.feature_importances_\n",
        "\n",
        "    # Create a simple DataFrame to show the results\n",
        "    feature_importance_df = pd.DataFrame(\n",
        "        list(zip(all_feature_names, importances)),\n",
        "        columns=['Feature', 'Importance']\n",
        "    ).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    print(\"Feature Importances (from final Random Forest model):\")\n",
        "    print(feature_importance_df.to_markdown(index=False, floatfmt=\".5f\"))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not calculate feature importances: {e}\")\n",
        "\n",
        "print(\"\\n--- PROJECT COMPLETE ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX9YpjJtauIB",
        "outputId": "34ecbb6a-72d9-4675-890f-b145e9556d28"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Libraries Imported ---\n",
            "Mounted at /content/drive\n",
            "--- Google Drive Mounted Successfully ---\n",
            "--- 3. Successfully loaded required columns from: /content/drive/MyDrive/Colab Notebooks/Dementia Prediction Dataset.csv ---\n",
            "--- 4. Engineering 'AGE' feature (VISITYR - BIRTHYR) ---\n",
            "--- 4. Feature Engineering Complete. ---\n",
            "--- 5. Cleaning Data... ---\n",
            "\n",
            "Filtered Target value counts (binary):\n",
            "target\n",
            "0    94933\n",
            "1    67754\n",
            "Name: count, dtype: int64\n",
            "--- Data Cleaned and Processed. ---\n",
            "--- 6. Building Preprocessing Pipelines... ---\n",
            "--- 7. Splitting Data into Train/Test... ---\n",
            "Training set size: 130149\n",
            "Test set size: 32538\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "Training and Tuning Random Forest (GridSearchCV)...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "Best Random Forest Params: {'model__max_depth': 20, 'model__min_samples_leaf': 2, 'model__n_estimators': 200}\n",
            "--- 8. Model training complete. ---\n",
            "\n",
            "\n",
            "==============================\n",
            "   MODEL EVALUATION RESULTS   \n",
            "==============================\n",
            "\n",
            "--- Logistic Regression (Baseline) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       0.63      0.82      0.72     18987\n",
            "Dementia (1)       0.58      0.33      0.42     13551\n",
            "\n",
            "    accuracy                           0.62     32538\n",
            "   macro avg       0.60      0.58      0.57     32538\n",
            "weighted avg       0.61      0.62      0.59     32538\n",
            "\n",
            "--- Tuned Random Forest (Final Model) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       0.67      0.78      0.72     18987\n",
            "Dementia (1)       0.59      0.46      0.52     13551\n",
            "\n",
            "    accuracy                           0.64     32538\n",
            "   macro avg       0.63      0.62      0.62     32538\n",
            "weighted avg       0.64      0.64      0.63     32538\n",
            "\n",
            "\n",
            "--- Model Comparison ---\n",
            "| Metric              |   Logistic Regression |   Tuned Random Forest |\n",
            "|:--------------------|----------------------:|----------------------:|\n",
            "| Accuracy            |                0.6199 |                0.6438 |\n",
            "| F1-Score (Dementia) |                0.4226 |                0.5164 |\n",
            "\n",
            "--- 10. Generating Feature Importances ---\n",
            "Feature Importances (from final Random Forest model):\n",
            "| Feature    |   Importance |\n",
            "|:-----------|-------------:|\n",
            "| AGE        |      0.42392 |\n",
            "| EDUC       |      0.33247 |\n",
            "| SEX_1      |      0.04075 |\n",
            "| SEX_2      |      0.03969 |\n",
            "| MARISTAT_1 |      0.03219 |\n",
            "| MARISTAT_3 |      0.01782 |\n",
            "| RACE_2     |      0.01254 |\n",
            "| MARISTAT_5 |      0.01242 |\n",
            "| RACE_1     |      0.01213 |\n",
            "| MARISTAT_2 |      0.01200 |\n",
            "| RACE_50    |      0.01106 |\n",
            "| HANDED_2   |      0.00890 |\n",
            "| HANDED_1   |      0.00851 |\n",
            "| HANDED_3   |      0.00673 |\n",
            "| RACE_5     |      0.00623 |\n",
            "| RACE_3     |      0.00479 |\n",
            "| MARISTAT_4 |      0.00397 |\n",
            "| MARISTAT_6 |      0.00391 |\n",
            "| HANDED_9   |      0.00387 |\n",
            "| RACE_99    |      0.00321 |\n",
            "| MARISTAT_9 |      0.00180 |\n",
            "| RACE_4     |      0.00111 |\n",
            "\n",
            "--- PROJECT COMPLETE ---\n"
          ]
        }
      ]
    }
  ]
}